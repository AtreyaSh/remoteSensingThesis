\section{Appendix 1: Field Data GPS Coordinates}\label{app4}

\begin{ThreePartTable}
	\centering
	\small
	\def\arraystretch{1.5}
	\begin{longtable}{L{2cm} L{6.3cm} L{6.3cm}}
		\caption{Coordinates of measured GPS points during field work}
		\hskip15pt	
		\label{tableApp2}\\
		\toprule[0.25mm]\\[-0.5cm]
		Point ID & Latitude (degrees/decimal-minutes, N) & Longitude (degrees/decimal-minutes, E) \\\\[-0.5cm]
		\midrule[0.35mm]\\[-0.6cm]	
01  &32$^\circ$ 10.104$'$& 76$^\circ$ 19.902$'$ \\
02  &32$^\circ$ 09.495$'$& 76$^\circ$ 24.896$'$ \\
03  &32$^\circ$ 10.898$'$& 76$^\circ$ 20.633$'$\\
04  &32$^\circ$ 10.254$'$& 76$^\circ$ 19.470$'$\\
05  &32$^\circ$ 10.341$'$& 76$^\circ$ 19.568$'$\\
06  & 32$^\circ$ 09.380$'$& 76$^\circ$ 19.134$'$\\
07  & 32$^\circ$ 09.450$'$& 76$^\circ$ 19.223$'$\\
08  & 32$^\circ$ 09.554$'$&  76$^\circ$ 19.160$'$\\
09  & 32$^\circ$ 09.360$'$&  76$^\circ$ 19.383$'$\\
10 &32$^\circ$ 09.315$'$&  76$^\circ$ 19.608$'$\\
11 &   32$^\circ$ 09.100$'$&  76$^\circ$ 20.350$'$\\
12 &32$^\circ$ 09.346$'$&  76$^\circ$ 21.373$'$\\
13 &32$^\circ$ 09.639$'$&  76$^\circ$ 21.806$'$\\
14 &32$^\circ$ 09.699$'$&  76$^\circ$ 21.797$'$\\
15 &32$^\circ$ 09.747$'$&  76$^\circ$ 22.117$'$\\
16 &32$^\circ$ 12.431$'$& 76$^\circ$ 22.627$'$\\
17 &32$^\circ$ 12.435$'$& 76$^\circ$ 22.654$'$\\
18 &32$^\circ$ 12.441$'$& 76$^\circ$ 22.677$'$\\
19 &32$^\circ$ 12.455$'$& 76$^\circ$ 22.712$'$\\
20 &32$^\circ$ 12.627$'$& 76$^\circ$ 23.025$'$\\
21 &32$^\circ$ 13.255$'$& 76$^\circ$ 23.436$'$\\
22 &32$^\circ$ 13.255$'$& 76$^\circ$ 23.436$'$\\
23 &32$^\circ$ 12.677$'$& 76$^\circ$ 22.317$'$\\
24 &32$^\circ$ 10.287$'$& 76$^\circ$ 19.536$'$\\
25 & 32$^\circ$ 09.603$'$& 76$^\circ$ 19.274$'$\\
26 & 32$^\circ$ 09.242$'$& 76$^\circ$ 20.504$'$\\
27 &  32$^\circ$ 09.630$'$& 76$^\circ$ 21.789$'$\\
28 &32$^\circ$ 12.479$'$& 76$^\circ$ 19.169$'$\\
29 &32$^\circ$ 12.459$'$& 76$^\circ$ 19.148$'$\\
30 &32$^\circ$ 12.153$'$& 76$^\circ$ 18.885$'$\\
31 &32$^\circ$ 12.179$'$& 76$^\circ$ 18.887$'$\\
32 &32$^\circ$ 12.213$'$& 76$^\circ$ 18.896$'$\\
\midrule[0.25mm]
\caption*{\raggedright\textit{*continued}} \hskip20pt\\
\midrule[0.25mm]\\[-0.5cm]
Point ID & Latitude (degrees/decimal-minutes, N) & Longitude (degrees/decimal-minutes, E) \\\\[-0.5cm]
\midrule[0.35mm]\\[-0.6cm]
33 &32$^\circ$ 12.234$'$& 76$^\circ$ 18.909$'$\\
34 &32$^\circ$ 12.217$'$& 76$^\circ$ 18.764$'$\\
35 &  32$^\circ$ 12.250$'$& 76$^\circ$ 18.843$'$\\
36 & 32$^\circ$ 12.185$'$& 76$^\circ$ 18.860$'$\\
37 &32$^\circ$ 13.902$'$& 76$^\circ$ 18.629$'$\\
38 &  32$^\circ$ 12.000$'$& 76$^\circ$ 18.601$'$\\
39 & 32$^\circ$ 14.002$'$& 76$^\circ$ 18.416$'$\\
40 &32$^\circ$ 14.871$'$& 76$^\circ$ 18.572$'$\\
41 &32$^\circ$ 14.875$'$& 76$^\circ$ 18.585$'$\\
42 &32$^\circ$ 14.869$'$& 76$^\circ$ 18.618$'$\\
43 &32$^\circ$ 14.850$'$& 76$^\circ$ 18.647$'$\\
44 &32$^\circ$ 14.838$'$& 76$^\circ$ 18.677$'$\\
45 &32$^\circ$ 14.819$'$& 76$^\circ$ 18.696$'$\\
46 &32$^\circ$ 15.299$'$& 76$^\circ$ 18.175$'$\\
47 &32$^\circ$ 13.926$'$& 76$^\circ$ 19.589$'$\\
48 & 32$^\circ$ 09.535$'$& 76$^\circ$ 24.922$'$\\
49 & 32$^\circ$ 09.595$'$& 76$^\circ$ 25.036$'$\\
50 &   32$^\circ$ 09.590$'$& 76$^\circ$ 25.070$'$\\
51 & 32$^\circ$ 09.592$'$& 76$^\circ$ 25.083$'$\\
52 & 32$^\circ$ 09.601$'$& 76$^\circ$ 25.120$'$\\
53 & 32$^\circ$ 09.812$'$& 76$^\circ$ 25.334$'$\\
54 & 32$^\circ$ 09.817$'$& 76$^\circ$ 25.307$'$\\
55 & 32$^\circ$ 09.856$'$& 76$^\circ$ 25.348$'$\\
56 & 32$^\circ$ 09.827$'$& 76$^\circ$ 25.365$'$\\
57 & 32$^\circ$ 09.942$'$& 76$^\circ$ 25.324$'$\\
58 & 32$^\circ$ 09.862$'$& 76$^\circ$ 25.329$'$\\
59 &32$^\circ$ 10.082$'$& 76$^\circ$ 25.512$'$\\
60 &32$^\circ$ 10.544$'$& 76$^\circ$ 25.061$'$\\
61 &32$^\circ$ 10.539$'$& 76$^\circ$ 25.005$'$\\
62 &32$^\circ$ 10.537$'$& 76$^\circ$ 24.935$'$\\
63 &32$^\circ$ 10.543$'$& 76$^\circ$ 24.894$'$\\
64 & 32$^\circ$ 10.560$'$& 76$^\circ$ 24.835$'$\\
65 &32$^\circ$ 10.165$'$& 76$^\circ$ 19.798$'$\\
\midrule[0.25mm]
\caption*{\raggedright\textit{*continued}} \hskip20pt\\
\midrule[0.25mm]\\[-0.5cm]
Point ID & Latitude (degrees/decimal-minutes, N) & Longitude (degrees/decimal-minutes, E) \\\\[-0.5cm]
\midrule[0.35mm]\\[-0.6cm]
66 &32$^\circ$ 10.173$'$& 76$^\circ$ 19.778$'$\\
67 &32$^\circ$ 10.123$'$& 76$^\circ$ 19.691$'$\\
68 & 32$^\circ$ 10.110$'$& 76$^\circ$ 19.689$'$\\
69 & 32$^\circ$ 10.116$'$& 76$^\circ$ 19.760$'$\\
70 & 32$^\circ$ 10.092$'$& 76$^\circ$ 19.750$'$\\
71 & 32$^\circ$ 09.615$'$& 76$^\circ$ 19.185$'$\\
72 & 32$^\circ$ 09.648$'$& 76$^\circ$ 19.239$'$\\
73 &  32$^\circ$ 09.650$'$& 76$^\circ$ 22.179$'$\\
74 & 32$^\circ$ 13.588$'$& 76$^\circ$ 21.772$'$\\
75 & 32$^\circ$ 12.777$'$& 76$^\circ$ 23.020$'$\\
76 &32$^\circ$ 12.899$'$& 76$^\circ$ 22.826$'$\\
77 &  32$^\circ$ 12.997$'$& 76$^\circ$ 22.850$'$\\
78 & 32$^\circ$ 13.001$'$& 76$^\circ$ 22.890$'$\\
79 &32$^\circ$ 13.067$'$& 76$^\circ$ 22.925$'$\\
80 & 32$^\circ$ 13.043$'$& 76$^\circ$ 22.898$'$\\
81 & 32$^\circ$ 13.095$'$& 76$^\circ$ 22.892$'$\\
82 &32$^\circ$ 13.109$'$& 76$^\circ$ 22.825$'$\\
		\bottomrule[0.25mm]
	\end{longtable}
\end{ThreePartTable}

\clearpage

\section{Appendix 2: GEE JavaScript Image Pre-Processing Algorithm}\label{app1}

\begin{figure}[H]
	\centering
	\footnotesize
	\begin{tikzpicture}[auto,
	block_center/.style ={rectangle, draw=black, thick, fill=white,
		text width=8em, text centered,
		minimum height=4em},
	block_left/.style ={rectangle, draw=black, thick, fill=white,
		text width=16em, text ragged, minimum height=4em, inner sep=6pt},
	block_noborder/.style ={rectangle, draw=none, thick, fill=none,
		text width=18em, text centered, minimum height=1em},
	block_assign/.style ={rectangle, draw=black, thick, fill=white,
		text width=18em, text ragged, minimum height=3em, inner sep=6pt},
	block_assign2/.style ={rectangle, draw=black, thick, fill=white,
		text width=10em, text ragged, minimum height=3em, inner sep=6pt},
	block_lost/.style ={rectangle, draw=black, thick, fill=white,
		text width=16em, text ragged, minimum height=3em, inner sep=6pt},
	line/.style ={draw, thick, -latex', shorten >=0pt}]
	% outlining the flowchart using the PGF/TikZ matrix funtion
	\matrix [column sep=18mm,row sep=9mm] {
		% enrollment - row 1
		\node [block_left] (landsat8import) {Import Landsat 8 SR Product into GEE code editor. Apply the following general filters: \\[0.2cm]
			1. WRS2 row/path: 147/38 \\
			2. Dates: 2013-01-01 - 2017-05-01 \\
			3. Cloud cover: $\leqslant$10 \\[0.2cm]
			Number of relevant images: 31};\\
		% enrollment - row 2
		\node [block_left] (landsat8clear) {Apply vector and QA filters: \\[0.2cm]
			1. Clip images to the polygon of the study area. \\
			2. Extract attributes of QA band. \\
			3. For each image, eliminate pixels affected by water, snow, clouds and cloud shadows. \\[0.2cm]
			End-products are clear images of the study area.}; \\
		\node [block_left] (landsat8hillshade) {Apply terrain and hillshade filters:\\[0.2cm]
			1. Import SRTM 30 m resolution DEM for study area. \\
			2. Eliminate pixels with elevations greater than or equal to 3,000 m. \\
			3. For each image, compute the specific hillshade given the solar azimuth and zenith. \\
			4. Eliminate pixels with hillshade less than or equal to 0.01. \\[0.2cm]
			End products are clear images with fewer pixels affected by rocky non-vegetated surfaces and terrain-related shadows.}; \\
		\node [block_left] (landsat8export) {Check and export resulting images: \\[0.2cm]
			1. Manually check all images for general quality. Remove any possibly compromised images. \\
			2. Export selected images in native EPSG:32643 coordinate reference system into Google Drive \\[0.2cm]
			
			Final number of images: 26}; \\
	};
	% end matrix
	% connecting nodes with paths
	\begin{scope}[every path/.style=line]
	% paths for enrollemnt rows
	\path (landsat8import) -- (landsat8clear);
	\path (landsat8clear) -- (landsat8hillshade);
	\path (landsat8hillshade) -- (landsat8export);
	\end{scope}
	\end{tikzpicture}
	\caption{Detailed flowchart for pre-processing Landsat 8 SR Data within the GEE code editor}\label{figApp1}
\end{figure}

\clearpage

\begin{minted}
[
framesep=2mm,
baselinestretch=1.2,
linenos,breaklines,
]{javascript}

// Variables "table" and "table2" were manually imported into GEE. "table" represents the study area polygon in the WGS84 CRS. "table2" represents the study area in the UTM 43N projection.

// Functions to calculate hillshade
function radians(img) {
return img.toFloat().multiply(Math.PI).divide(180);
}

function hillshade(az, ze, slope, aspect) {
var azimuth = radians(ee.Image(az));
var zenith = radians(ee.Image(ze));
return azimuth.subtract(aspect).cos()
.multiply(slope.sin())
.multiply(zenith.sin())
.add(
zenith.cos().multiply(slope.cos()));
}

// Filtering Landast 8 SR ImageCollection
var collection = ee.ImageCollection('LANDSAT/LC8_SR')
.filter(ee.Filter.eq('WRS_PATH', 147))
.filter(ee.Filter.eq('WRS_ROW', 38))
.filterDate('2011-01-01', '2017-05-01')
.filter(ee.Filter.lte('CLOUD_COVER', 10))
.map(function(image) {
return image.clipToCollection(table2);
})
.map(function(image){
var clear = image.select('cfmask').eq(0);
clear = clear.updateMask(clear);
return image.updateMask(clear);
})
.map(function(image){
var imageClipped = image3.clip(table);
var azimuthImage = image.metadata('solar_azimuth_angle');
var zenithImage = image.metadata('solar_zenith_angle');
var terrain = ee.Algorithms.Terrain(imageClipped);
var slope = radians(terrain.select('slope'));
var aspect = radians(terrain.select('aspect'));
var hillshadeExp = hillshade(azimuthImage, zenithImage, slope, aspect);
var hillshadeExpFilter = hillshadeExp.select(0).gt(0.01);
hillshadeExpFilter = hillshadeExpFilter.updateMask(hillshadeExpFilter);
var noList = ee.List([]);
var float = image.projection().transform().split(', ');
for(var i = 6; i <= 12; i += 2){
var no = ee.Number.parse(ee.String(float.get(i)).replace(']', '').replace(']', ''));
no = no.toFloat();
noList = noList.add(no);
}
var noList2 = noList.insert(1, 0).insert(3, 0);
var hillshadeExpFilterRP = hillshadeExpFilter.reproject('EPSG:32643', noList2);
return image.updateMask(hillshadeExpFilterRP);
})
.select(['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']);

// Function/command to bulk export filtered ImageCollection
var ExportCol = function(col, folder, scale,
nimg, maxPixels, region) {
nimg = nimg || 31;
scale = scale || 30;
maxPixels = maxPixels || 1e10;
var colList = col.toList(nimg);
var n = colList.size().getInfo();
for (var i = 0; i < n; i++) {
var img = ee.Image(colList.get(i));
var id = img.id().getInfo();
region = table2;
Export.image.toDrive({
image:img,
description: id + '_' + i,
folder: folder,
region: region,
scale: scale,
maxPixels: maxPixels,
});
}
};

ExportCol(collection, "Landsat_Export", 30);

\end{minted}

\clearpage

\section{Appendix 3: R Vegetation Cover Classification Algorithm}\label{app2}

\begin{figure}[H]
	\centering
	\footnotesize
	\begin{tikzpicture}[auto,
	block_center/.style ={rectangle, draw=black, thick, fill=white,
		text width=10em, text centered,
		minimum height=4em},
	block_left/.style ={rectangle, draw=black, thick, fill=white,
		text width=16em, text ragged, minimum height=4em, inner sep=6pt},
	block_noborder/.style ={rectangle, draw=none, thick, fill=none,
		text width=18em, text centered, minimum height=1em},
	block_assign/.style ={rectangle, draw=black, thick, fill=white,
		text width=18em, text ragged, minimum height=3em, inner sep=6pt},
	block_assign2/.style ={rectangle, draw=black, thick, fill=white,
		text width=10em, text ragged, minimum height=3em, inner sep=6pt},
	block_lost/.style ={rectangle, draw=black, thick, fill=white,
		text width=16em, text ragged, minimum height=3em, inner sep=6pt},
	line/.style ={draw, thick, -latex', shorten >=0pt}
	]
	% outlining the flowchart using the PGF/TikZ matrix funtion
	\matrix [column sep=35mm,row sep=9mm] {
		% enrollment - row 1
		\node [block_left] (landsat8import) {Import the following datasets: \\[0.2cm]
			1. CRAN "rgdal", "raster", "snow" and "caret" packages.\\
			2. Pre-processed 26 images produced in GEE \\
			3. Shape files for revised vegetation class polygons in  native UTM Zone 43N projection.};
		% Apply for loop here to show with special arrows, for each image specifically
		& \node [block_left] (landsat8clear) {Apply the following sampling operations: \\[0.2cm]
			1. Extract and store the 7 band values for pixels that coincide with the vegetation class polygons. \\
			2. Partition the extracted values randomly into training and test datasets with 70-30 ratio respectively. \\
			3. Conduct undersampling procedure on training dataset in order to make balanced dataset. Number of pixels for each class should be the same as the lowest number for any given class.}; \\
		&\node [block_left] (landsat8hillshade) {Train the random forests algorithm with the corresponding balanced training dataset. In particular:\\[0.2cm]
			1. Set number of trees = 1000. \\
			2. Record out-of-bag accuracy parameters and optimized $m$ variable}; \\
		&\node [block_left] (landsat8export) {Validate trained random forests model on pre-defined test dataset using a confusion matrix. Record accuracy parameters such as overall accuracy, Kappa coefficient and producer's accuracy by class.}; \\
		&\node [block_left] (landsat8next) {Use trained random forests model to predict vegetation cover classes of entire image corresponding to study area.}; \\\\[-0.3cm]
		&\node [block_center] (landsat8final) {Export resulting vegetation cover classification image.}; \\
	};
	% end matrix
	% connecting nodes with paths
	\begin{scope}[every path/.style=line]
	% paths for enrollemnt rows
	\path (landsat8import) -- node [text width=2.5cm,midway,above=0.5em,align=center] {For each image from $i=1$ to $i=26$; $i\in \mathbb{N}$} (landsat8clear);
	\path (landsat8import) -- coordinate[midway](m) (landsat8clear);
	\path (landsat8clear) -- (landsat8hillshade);
	\path (landsat8hillshade) -- (landsat8export);
	\path (landsat8export) -- (landsat8next);
	\path (landsat8next) -- coordinate[midway](k) (landsat8final);
	%	\path (landsat8next) |- ($(landsat8next.south west) + (-1.78,-0.7)$) |- +(0,10cm) ;
	\end{scope}
	\path (k) + (-4.6cm, 0cm) coordinate (B);
	\draw[thick] (k) |- (B);
	\draw[-latex', shorten >= -2.5pt, thick] (B) |-  +(0cm, 13.91cm) node [pos=0.25, left = 0.23cm] {Iterate};
	%	\draw[thick] (k) |- \coordinate(s) +(-4.6cm, 0);
	%	\draw[line] (0,0) |- +(-4.6cm, 13.98cm);
	\end{tikzpicture}
	\vspace{5pt}
	\caption{Detailed flowchart for vegetation cover classification methodology}\label{figApp2}
\end{figure}

 \clearpage
 
\begin{minted}
[
framesep=2mm,
baselinestretch=1.2,
linenos,breaklines,
]{R}

# Set working directory
setwd("<working directory>")

# Load libraries needed, assuming they are installed
library(rgdal)
library(raster)
library(caret)
library(snow)

# Import training images and stack them training RF model
rlist<-list.files(path="<source directory>", pattern="tif$", full.names = TRUE) 
s<-lapply(rlist, stack)

# Import shapefile containing vegetation polygons in native resolution
shape_pointData <- shapefile("<file destination>")
responseCol <- "OBJECTID"

# Import polygon for masking prediction image(s)
dLower <- shapefile("<file destination>")

# Start of for-loop for processing RF model on images
for (j in 1:length(s)){

training_image <- s[[j]]

# To make band names shorter and create a smaller prediction image
names(training_image) <- c(paste0("B", 1:7, coll = ""))
prediction_image <- mask(training_image, dLower)

# Extract values of raster pixels based on vegetation class polygons
image_dfall = data.frame(matrix(vector(), nrow = 0, ncol = length(names(training_image)) + 1))
for (i in 1:length(unique(shape_pointData[[responseCol]]))){
category <- unique(shape_pointData[[responseCol]])[i]
categorymap <- shape_pointData[shape_pointData[[responseCol]] == category,]
dataSet <- extract(training_image, categorymap)
dataSet <- dataSet[!unlist(lapply(dataSet, is.null))]

if(is(shape_pointData, "SpatialPointsDataFrame")){
dataSet <- cbind(dataSet, class = as.numeric(category))
image_dfall <- rbind(image_dfall, dataSet)
}

if(is(shape_pointData, "SpatialPolygonsDataFrame")){
dataSet <- lapply(dataSet, function(x){cbind(x, class = as.numeric(rep(category, nrow(x))))})
df <- do.call("rbind", dataSet)
image_dfall <- rbind(image_dfall, df)
}
}

# To create data partition for training and test dataset
image_inBuild <- createDataPartition(y = image_dfall$class, p = 0.7, list = FALSE)
image_train <- image_dfall[image_inBuild,] #training data
image_train <- image_train[complete.cases(image_train), ]
image_valid <- image_dfall[-image_inBuild,] #test data, a.k.a validation data
image_valid <- image_valid[complete.cases(image_valid), ]

# To undersample training dataset for balanced training data
undersample_ds <- function(x, classCol, nsamples_class){
for (k in 1:length(unique(x[, classCol]))){
class.k <- unique(x[, classCol])[k]
if((sum(x[, classCol] == class.k) - nsamples_class) != 0){
x <- x[-sample(which(x[, classCol] == class.k), 
sum(x[, classCol] == class.k) - nsamples_class), ]
}
}
return(x)
}

training_bc <- undersample_ds(image_train, "class", min(table(image_train$class)))

# Building the model with data partition, all bands, 1000 trees and variable importance tracking
imagemod_rf_1k <- train(as.factor(class) ~ B1 + B2 + B3 + B4 + B5 + B6 + B7, method = "rf", data = training_bc, ntree = 1000, importance = TRUE)

# Save the trained model
saveRDS(imagemod_rf_1k, names(s[[j]])[1])

# Shows paramters like accuracy and kappa coefficient for internal OOB data
imagemod_rf_1k

# View the variable importance of the RF model
varImp(imagemod_rf_1k)[1]

# Prediction on test dataset based on trained model
imagepred_valid_1k <- predict(imagemod_rf_1k, image_valid)

# Confusion matrix of trained model on test dataset; provides accuracy
confusionMatrix1 <- confusionMatrix(imagepred_valid_1k, image_valid$class)$overall[1]

# Confusion matrix of trained model on test dataset; provides kappa coefficient
confusionMatrix3 <- confusionMatrix(imagepred_valid_1k, image_valid$class)$overall[2]

# Confusion matrix of trained model on test dataset; provides user's accuracy by class
confusionMatrix2 <- confusionMatrix(imagepred_valid_1k, image_valid$class)$byClass[,1]

# Apply the RF model to predict the entire image
beginCluster()
preds_rf2_1k <- clusterR(prediction_image, raster::predict, args = list(model = imagemod_rf_1k))
endCluster()

# Save predicted image
writeRaster(preds_rf2_1k, file.path("<target directory>", names(s[[j]])[1]), format = "GTiff", overwrite = TRUE)
}

\end{minted}

\section{Appendix 4: R Vegetation Cover Loss Analysis Algorithm}\label{app3}

\begin{figure}[H]
	\centering
	\footnotesize
	\begin{tikzpicture}[auto,
	block_center/.style ={rectangle, draw=black, thick, fill=white,
		text width=10em, text centered,
		minimum height=4em},
	block_left/.style ={rectangle, draw=black, thick, fill=white,
		text width=16em, text ragged, minimum height=4em, inner sep=6pt},
	block_noborder/.style ={rectangle, draw=none, thick, fill=none,
		text width=18em, text centered, minimum height=1em},
	block_assign/.style ={rectangle, draw=black, thick, fill=white,
		text width=18em, text ragged, minimum height=3em, inner sep=6pt},
	block_assign2/.style ={rectangle, draw=black, thick, fill=white,
		text width=10em, text ragged, minimum height=3em, inner sep=6pt},
	block_lost/.style ={rectangle, draw=black, thick, fill=white,
		text width=16em, text ragged, minimum height=3em, inner sep=6pt},
	line/.style ={draw, thick, -latex', shorten >=0pt}
	]
	% outlining the flowchart using the PGF/TikZ matrix funtion
	\matrix [column sep=35mm,row sep=9mm] {
		% enrollment - row 1
		\node [block_left] (landsat8import) {
			Import the following datasets: \\[0.2cm]
			1. CRAN "rgdal", "raster", "MASS" and "igraph" packages.\\[0.1cm]
			2. Vegetation cover classification images in their 4 respective groups. For simplicity, rename the groups from 2013, 2014, 2015 and 2016 to 1, 2, 3 and 4 respectively. \\[0.3cm]
			Apply the following operations: \\[0.2cm]
			1. Remove entire stacks of pixels if they contain less than 13 out of 26 pixels per stack for the entire time period. \\[0.1cm]
			2. For the remaining pixel stacks containing sufficient data, calculate the median values of each pixel stack to give the overall median vegetation classification image.};
		% Apply for loop here to show with special arrows, for each image specifically
		& \node [block_left] (landsat8clear) {Apply the following operations: \\[0.2cm]
			1. Import the $k$th and $(k+1)$th data groups as a group-pair. \\[0.1cm]
			2. Remove pixel stacks in the group-pair if more than half of the total pixels per stack per group are NA values. \\[0.1cm]
			3. With the remaining pixel stacks, calculate the medians of all pixel stacks to give median vegetation classification images for the $k$th and $(k+1)$th groups.\\[0.1cm]
			4. Find the median difference image by subtracting the $k$th data group median from the $(k+1)$th data group median \\[0.1cm]
			5. Retain pixel stacks where the median difference has values greater than or equal to 1.}; \\[-0.3cm]
		&\node [block_left] (landsat8hillshade) {From the resulting median difference image, apply the following operations:\\[0.2cm]
			1. Remove pixels stacks which had a median value of 2 in the $(k+1)$th data group. \\[0.1cm]
			2. Conduct the one-tailed Mann-Whitney $U$ test on the remaining pixels stacks with an alternative hypothesis of a significant increase from the $k$th to the $(k+1)$th group.\\[0.1cm]
			3. Filter pixel stacks with a p-value $\leq$ 0.05}; \\\\[-0.3cm]
		&\node [block_left] (landsat8final) {Apply the following operations:\\[0.3cm]
			1. Export the resulting flagged pixels corresponding to the significant increase between the $k$th and $(k+1)$th groups. Let these pixels be known as "raw" pixels. Mosaic all flagged raw pixels to get a sum of raw pixels.\\[0.1cm]
			2. Clump resulting pixels based on 8-directional spatial criteria to produce clump filters of flagged pixels. Let these pixels be known as "clump" pixels. Mosaic all flagged clump pixels to get a sum of clump pixels.};\\
	};
	% end matrix
	% connecting nodes with paths
	\begin{scope}[every path/.style=line]
	% paths for enrollemnt rows
	\path (landsat8import) -- node [text width=2.5cm,midway,above=0.5em,align=center] {For variables from $k=1$ to $k=3$; $k \in \mathbb{N}$} (landsat8clear);
	\path (landsat8import) -- coordinate[midway](m) (landsat8clear);
	\path (landsat8clear) -- (landsat8hillshade);
	\path (landsat8hillshade) -- coordinate[midway](k) (landsat8final);
	%	\path (landsat8next) |- ($(landsat8next.south west) + (-1.78,-0.7)$) |- +(0,10cm) ;
	\end{scope}
	\path (k) + (-4.6cm, 0cm) coordinate (B);
	\draw[thick] (k) |- (B);
	\draw[-latex', shorten >= -2.5pt, thick] (B) |- +(0cm, 11.17cm) node [pos=0.18, left = 0.20cm] {Iterate};
	%	\draw[thick] (k) |- \coordinate(s) +(-4.6cm, 0);
	%	\draw[line] (0,0) |- +(-4.6cm, 13.98cm);
	\end{tikzpicture}
	\vspace{5pt}
	\caption{Detailed flowchart for vegetation cover loss analysis}\label{figApp3}
\end{figure}

\begin{minted}
[
framesep=2mm,
baselinestretch=1.2,
linenos,breaklines,
]{R}

# Set up working directory
setwd("<working directory>")

# Libraries needed
library(rgdal)
library(raster)
library(snow)
library(MASS)
library(igraph)

# Import and variables definition
rlist<-list.files(path="<source directory>", pattern="tif$", full.names = TRUE) 
s<- stack(rlist)
s1 <- stack(rlist[1:14])
s2 <- stack(rlist[7:21])
s3 <- stack(rlist[15:26])
s2013 <- stack(rlist[1:6])
s2014 <- stack(rlist[7:14])
s2015 <- stack(rlist[15:21])
s2016 <- stack(rlist[22:26])
nacount <- sum(is.na(s))
nacount2013 <- sum(is.na(s2013))
nacount2014 <- sum(is.na(s2014))
nacount2015 <- sum(is.na(s2015))
nacount2016 <- sum(is.na(s2016))

# Overall median generation
for(i in 1:length(names(s)))
{
s[[i]][nacount[] >= 14] <- NA
}

beginCluster()
tsSortM <- calc(s, fun=median, na.rm = T)
endCluster()

# Annual median, median difference and buffer generation
for(i in 1:length(names(s2013)))
{
s2013[[i]][nacount2013[] >= 4] <- NA
}
for(i in 1:length(names(s2014)))
{
s2014[[i]][nacount2014[] >= 5] <- NA
}
for(i in 1:length(names(s2015)))
{
s2015[[i]][nacount2015[] >= 4] <- NA
}
for(i in 1:length(names(s2016)))
{
s2016[[i]][nacount2016[] >= 3] <- NA
}

beginCluster()
tsM2013 <- calc(s2013, fun=median, na.rm = T)
tsM2014 <- calc(s2014, fun=median, na.rm = T)
tsM2015 <- calc(s2015, fun=median, na.rm = T)
tsM2016 <- calc(s2016, fun=median, na.rm = T)
mstack1 <- stack(tsM2013, tsM2014)
mstack2 <- stack(tsM2014, tsM2015)
mstack3 <- stack(tsM2015, tsM2016)
subtract <- function(x){
d = x[[2]]-x[[1]]
return(d)
}
tsMD_2013_2014 <- calc(mstack1, fun=subtract)
tsMD_2014_2015 <- calc(mstack2, fun=subtract)
tsMD_2015_2016 <- calc(mstack3, fun=subtract)
endCluster()

buffer1 <- tsMD_2013_2014; buffer1[buffer1[] < 1] <- NA
buffer2 <- tsMD_2014_2015; buffer2[buffer2[] < 1] <- NA
buffer3 <- tsMD_2015_2016; buffer3[buffer3[] < 1] <- NA
buffer1[tsM2014[] <= 2] <- NA
buffer2[tsM2015[] <= 2] <- NA
buffer3[tsM2016[] <= 2] <- NA

# Cleaning up groups for U-test
for(i in 1:6)
{
s1[[i]][nacount2013[] >= 4] <- NA
}
for(j in 7:14)
{
s1[[j]][nacount2014[] >= 5] <- NA
}
for(i in 1:8)
{
s2[[i]][nacount2014[] >= 5] <- NA
}
for(j in 9:15)
{
s2[[j]][nacount2015[] >= 4] <- NA
}
for(i in 1:7)
{
s3[[i]][nacount2015[] >= 4] <- NA
}
for(j in 8:12)
{
s3[[i]][nacount2016[] >= 3] <- NA
}

# Applying U-test on buffer stack
buffer <- stack(buffer1, buffer2, buffer3)

customUTest <- function(x, i, n1, n2){
extract <- extract(buffer[[i]], c(1:length(buffer[[i]])))
cellno <- which(!is.na(extract) == TRUE)
pb <- txtProgressBar(min = 0, max = length(cellno), initial = 0, char = "=",
width = NA, title, label, style = 3, file = "")

for(k in 1:length(cellno)){
e1 <- as.vector(extract(x[[1:n1]], c(cellno[k])))
e1 <- e1[!is.na(e1)]
e2 <- as.vector(extract(x[[(n1+1):n2]], c(cellno[k])))
e2 <- e2[!is.na(e2)]
p <- wilcox.test(e1, e2, alternative = "less")$p.value
buffer[[i]][cellno[k]] <- p
Sys.sleep(0.1)
setTxtProgressBar(pb, k, title = NULL, label = NULL)
}
return(buffer[[i]])
close(pb)
}

beginCluster()
test2013_2014 <- customUTest(s1, i=1, n1=6, n2 = 14)
test2014_2015 <- customUTest(s2, i=2, n1=8, n2 = 15)
test2015_2016 <- customUTest(s3, i=3, n1=7, n2 = 12)
endCluster()

# Filter p-value to obtain raw pixels
test2013_2014_Raw <- test2013_2014; test2013_2014_Raw [test2013_2014_Raw[] > 0.05] <- NA
test2014_2015_Raw <- test2014_2015; test2014_2015_Raw[test2014_2015_Raw[] > 0.05] <- NA
test2015_2016_Raw <- test2015_2016; test2015_2016_Raw[test2015_2016_Raw[] > 0.05] <- NA

# Filter raw pixels to obtain clump pixels
test_Clump <- stack(test2013_2014_Raw, test2014_2015_Raw, test2015_2016_Raw)

for(i in 1:3){
testClump <- clump(test_Clump[[i]], directions=8, gaps = TRUE)
clumpFreq <- freq(testClump)
clumpFreq <- as.data.frame(clumpFreq)
excludeID <- clumpFreq$value[which(clumpFreq$count==1)]
test_Clump[[i]][testClump %in% excludeID] <- NA
}

test2013_2014_Clump <- test_Clump[[1]]
test2014_2015_Clump <- test_Clump[[2]]
test2015_2016_Clump <- test_Clump[[3]]

\end{minted}